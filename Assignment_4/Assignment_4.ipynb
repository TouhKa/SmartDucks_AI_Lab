{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgvvWbigDxeN",
    "outputId": "5e662a23-27ce-4ad8-e1c2-92d220aec7e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards, round):\n",
    "    plt.plot(range(0,round + 1), rewards)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Mean Reward\")\n",
    "    plt.title(\"Mean Reward per Episode\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "Tz99vn4VMqOd",
    "outputId": "8fbd44ef-54ae-43e4-efb9-d85d8d332126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round -1 :Initial mean reward -38.51850381932774 --- Initial top 20 reward is 124.91011414386193\n",
      "Start training...\n",
      "Round 0: Current mean reward -33.94490274949048 --- Top 20 reward is 88.46014947498502\n",
      "Round 1: Current mean reward -44.58101711947396 --- Top 20 reward is 97.02556082724848\n",
      "Round 2: Current mean reward -54.06519482221959 --- Top 20 reward is 90.29880349830384\n",
      "Round 3: Current mean reward -46.645250066149174 --- Top 20 reward is 34.395083193248425\n",
      "Round 4: Current mean reward -36.78557781778498 --- Top 20 reward is 76.63841571189381\n",
      "Round 5: Current mean reward -36.02611245592557 --- Top 20 reward is 128.10370383772022\n",
      "Round 6: Current mean reward -26.31300868940332 --- Top 20 reward is 175.76649915907686\n",
      "Round 7: Current mean reward -36.28940858553412 --- Top 20 reward is 52.10801583569715\n",
      "Round 8: Current mean reward -42.679804633705814 --- Top 20 reward is 115.21950214671224\n",
      "Round 9: Current mean reward -37.120619946072935 --- Top 20 reward is 96.83774965990631\n",
      "Round 10: Current mean reward -35.40908554789556 --- Top 20 reward is 94.38597962928367\n",
      "Round 11: Current mean reward -36.735787879141185 --- Top 20 reward is 129.97651702848663\n",
      "Round 12: Current mean reward -36.62623400911895 --- Top 20 reward is 72.28908459294294\n",
      "Round 13: Current mean reward -34.973244948842726 --- Top 20 reward is 112.41285714032014\n",
      "Round 14: Current mean reward -39.09598564799684 --- Top 20 reward is 107.11794526001788\n",
      "Round 15: Current mean reward -40.877664232864255 --- Top 20 reward is 120.46108980787817\n",
      "Round 16: Current mean reward -37.095077270609686 --- Top 20 reward is 97.21209898337403\n",
      "Round 17: Current mean reward -42.788956736747956 --- Top 20 reward is 117.73879325244506\n",
      "Round 18: Current mean reward -40.12503154286091 --- Top 20 reward is 98.79435834450332\n",
      "Round 19: Current mean reward -41.17126410314557 --- Top 20 reward is 89.13267321270746\n",
      "Round 20: Current mean reward -41.54166218882393 --- Top 20 reward is 88.32285468323914\n",
      "Round 21: Current mean reward -45.87681472149567 --- Top 20 reward is 123.6323818382119\n",
      "Round 22: Current mean reward -44.75583578481342 --- Top 20 reward is 131.15575109789452\n",
      "Round 23: Current mean reward -50.0530466284435 --- Top 20 reward is 61.58069783159814\n",
      "Round 24: Current mean reward -56.57764058553043 --- Top 20 reward is 99.20461572708015\n",
      "Round 25: Current mean reward -54.19525070572948 --- Top 20 reward is 122.44131301044635\n",
      "Round 26: Current mean reward -55.060632414096766 --- Top 20 reward is 103.59496087492676\n",
      "Round 27: Current mean reward -56.95051405130854 --- Top 20 reward is 116.2325807568482\n",
      "Round 28: Current mean reward -64.55544735096589 --- Top 20 reward is 118.26571992389124\n",
      "Round 29: Current mean reward -59.3749033469068 --- Top 20 reward is 108.56326152485481\n",
      "Round 30: Current mean reward -59.939502649082186 --- Top 20 reward is 108.44168401011427\n",
      "Round 31: Current mean reward -61.18776549382973 --- Top 20 reward is 90.96471505767366\n",
      "Round 32: Current mean reward -57.66653317720804 --- Top 20 reward is 109.23169874016301\n",
      "Round 33: Current mean reward -56.00721801907482 --- Top 20 reward is 114.4268860586546\n",
      "Round 34: Current mean reward -53.21646525819972 --- Top 20 reward is 111.24660958942657\n",
      "Round 35: Current mean reward -55.27050312449321 --- Top 20 reward is 99.75385656439684\n",
      "Round 36: Current mean reward -57.93371109371867 --- Top 20 reward is 107.93104141027004\n",
      "Round 37: Current mean reward -50.37105129576826 --- Top 20 reward is 105.07507078144029\n",
      "Round 38: Current mean reward -50.94505822203829 --- Top 20 reward is 95.57915551300684\n",
      "Round 39: Current mean reward -48.59342952171205 --- Top 20 reward is 131.37705251273042\n",
      "Round 40: Current mean reward -46.6970509638081 --- Top 20 reward is 111.76106586364153\n",
      "Round 41: Current mean reward -45.314972085497494 --- Top 20 reward is 115.50425348707793\n",
      "Round 42: Current mean reward -43.63356009591114 --- Top 20 reward is 110.45125282732977\n",
      "Round 43: Current mean reward -44.556759106191066 --- Top 20 reward is 120.54856688265527\n",
      "Round 44: Current mean reward -43.31063501045731 --- Top 20 reward is 101.07630029679163\n",
      "Round 45: Current mean reward -45.9521978433656 --- Top 20 reward is 142.97927719040553\n",
      "Round 46: Current mean reward -48.24304221872043 --- Top 20 reward is 92.13105766182949\n",
      "Round 47: Current mean reward -35.9414682663524 --- Top 20 reward is 142.6837532624303\n",
      "Round 48: Current mean reward -42.728338443140444 --- Top 20 reward is 138.3377941082188\n",
      "Round 49: Current mean reward -41.56058691501005 --- Top 20 reward is 148.11225480138373\n",
      "Round 50: Current mean reward -43.591031271536025 --- Top 20 reward is 118.3142798425619\n",
      "Round 51: Current mean reward -39.52873964559022 --- Top 20 reward is 135.94914408165485\n",
      "Round 52: Current mean reward -41.21289112118038 --- Top 20 reward is 138.58296599528654\n",
      "Round 53: Current mean reward -40.21158576358041 --- Top 20 reward is 143.85153318927595\n",
      "Round 54: Current mean reward -37.95556333487089 --- Top 20 reward is 144.5002798352829\n",
      "Round 55: Current mean reward -36.09299910592823 --- Top 20 reward is 152.84786393818445\n",
      "Round 56: Current mean reward -39.226110580907694 --- Top 20 reward is 143.64838236881127\n",
      "Round 57: Current mean reward -34.08668748612766 --- Top 20 reward is 131.16090901885062\n",
      "Round 58: Current mean reward -34.943360297929615 --- Top 20 reward is 136.1636840701563\n",
      "Round 59: Current mean reward -36.880213961827884 --- Top 20 reward is 126.77342633501851\n",
      "Round 60: Current mean reward -35.34618480352423 --- Top 20 reward is 75.77878926088052\n",
      "Round 61: Current mean reward -33.44020296491485 --- Top 20 reward is 114.00511853863024\n",
      "Round 62: Current mean reward -32.23121635955078 --- Top 20 reward is 99.82060293816596\n",
      "Round 63: Current mean reward -31.621298482568115 --- Top 20 reward is 130.0674343426273\n",
      "Round 64: Current mean reward -37.7943902843505 --- Top 20 reward is 126.93746135185127\n",
      "Round 65: Current mean reward -35.05315009918058 --- Top 20 reward is 118.95687143896915\n",
      "Round 66: Current mean reward -36.41353207603949 --- Top 20 reward is 148.21265010621667\n",
      "Round 67: Current mean reward -39.335935476904496 --- Top 20 reward is 130.726389122345\n",
      "Round 68: Current mean reward -33.60726932384294 --- Top 20 reward is 144.73195370607493\n",
      "Round 69: Current mean reward -39.13459545501404 --- Top 20 reward is 126.68116491504688\n",
      "Round 70: Current mean reward -38.462766673345925 --- Top 20 reward is 125.65260596225832\n",
      "Round 71: Current mean reward -37.904046797469334 --- Top 20 reward is 142.7619934519937\n",
      "Round 72: Current mean reward -34.292666384663114 --- Top 20 reward is 149.1696724643379\n",
      "Round 73: Current mean reward -39.799954174307324 --- Top 20 reward is 143.56156982957017\n",
      "Round 74: Current mean reward -40.497725006166554 --- Top 20 reward is 102.95073447898248\n",
      "Round 75: Current mean reward -35.31522827803482 --- Top 20 reward is 150.00640966298496\n",
      "Round 76: Current mean reward -34.68568427703321 --- Top 20 reward is 149.29653519982284\n",
      "Round 77: Current mean reward -22.778159139972477 --- Top 20 reward is 147.54138329611368\n",
      "Round 78: Current mean reward -36.652161333963015 --- Top 20 reward is 144.50270053792835\n",
      "Round 79: Current mean reward -37.914425715907065 --- Top 20 reward is 122.49666644339193\n",
      "Round 80: Current mean reward -35.91432116015371 --- Top 20 reward is 144.58562043764846\n",
      "Round 81: Current mean reward -36.097944605318894 --- Top 20 reward is 138.34393793361633\n",
      "Round 82: Current mean reward -37.93079352567114 --- Top 20 reward is 137.58223428244406\n",
      "Round 83: Current mean reward -38.35147541226814 --- Top 20 reward is 141.54067924561062\n",
      "Round 84: Current mean reward -37.635717337075846 --- Top 20 reward is 145.51323556004712\n",
      "Round 85: Current mean reward -38.396797193159244 --- Top 20 reward is 146.1822579904091\n",
      "Round 86: Current mean reward -29.928973186411955 --- Top 20 reward is 164.00123236538397\n",
      "Round 87: Current mean reward -41.002263299150144 --- Top 20 reward is 137.70097556811524\n",
      "Round 88: Current mean reward -38.01796134570001 --- Top 20 reward is 137.47744991446763\n",
      "Round 89: Current mean reward -42.17873578664179 --- Top 20 reward is 140.76352958714386\n",
      "Round 90: Current mean reward -28.557190881132556 --- Top 20 reward is 157.95252767558645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 91: Current mean reward -37.642465779347035 --- Top 20 reward is 148.25741356670534\n",
      "Round 92: Current mean reward -36.548842729048715 --- Top 20 reward is 146.29145576904048\n",
      "Round 93: Current mean reward -28.299784285095317 --- Top 20 reward is 162.85543728304074\n",
      "Round 94: Current mean reward -40.64539809005084 --- Top 20 reward is 139.2946160901497\n",
      "Round 95: Current mean reward -36.33415485342934 --- Top 20 reward is 145.13829391593947\n",
      "Round 96: Current mean reward -27.97968034544981 --- Top 20 reward is 167.0014379200761\n",
      "Round 97: Current mean reward -33.1704996543535 --- Top 20 reward is 144.48365064186\n",
      "Round 98: Current mean reward -40.72429233638389 --- Top 20 reward is 152.8400407954651\n",
      "Round 99: Current mean reward -40.6370942745566 --- Top 20 reward is 129.59808099126283\n",
      "Round 100: Current mean reward -39.95226172659675 --- Top 20 reward is 148.64071351723578\n",
      "Round 101: Current mean reward -38.80135583943914 --- Top 20 reward is 145.9983730734172\n",
      "Round 102: Current mean reward -43.79884411759107 --- Top 20 reward is 155.47262528151452\n",
      "Round 103: Current mean reward -34.9340130035607 --- Top 20 reward is 147.43875996197352\n",
      "Round 104: Current mean reward -34.82201612386606 --- Top 20 reward is 153.5771117897049\n",
      "Round 105: Current mean reward -37.88794012882787 --- Top 20 reward is 157.49150696693815\n",
      "Round 106: Current mean reward -40.02418186700692 --- Top 20 reward is 145.93639607719462\n",
      "Round 107: Current mean reward -50.69587851725249 --- Top 20 reward is 133.48940673653547\n",
      "Round 108: Current mean reward -47.775091262994536 --- Top 20 reward is 152.56534843896884\n",
      "Round 109: Current mean reward -43.21747390930885 --- Top 20 reward is 151.9902961611367\n",
      "Round 110: Current mean reward -38.48973364621793 --- Top 20 reward is 156.5568586034236\n",
      "Round 111: Current mean reward -39.3204629410506 --- Top 20 reward is 152.73759910734802\n",
      "Round 112: Current mean reward -43.28565342861221 --- Top 20 reward is 148.12160479408968\n",
      "Round 113: Current mean reward -40.96754490568235 --- Top 20 reward is 146.06967479341978\n",
      "Round 114: Current mean reward -38.62717498733867 --- Top 20 reward is 160.69709428185328\n",
      "Round 115: Current mean reward -41.858334142406356 --- Top 20 reward is 152.71381436832291\n",
      "Round 116: Current mean reward -44.052066476492946 --- Top 20 reward is 152.53397600150103\n",
      "Round 117: Current mean reward -44.8325290180805 --- Top 20 reward is 145.58131474783036\n",
      "Round 118: Current mean reward -47.08492848374255 --- Top 20 reward is 149.12626887391562\n",
      "Round 119: Current mean reward -49.90652447131631 --- Top 20 reward is 135.1424825544247\n",
      "Round 120: Current mean reward -46.52757764122285 --- Top 20 reward is 135.8134805916783\n",
      "Round 121: Current mean reward -45.67440103460695 --- Top 20 reward is 153.55244393062407\n",
      "Round 122: Current mean reward -47.241897980369856 --- Top 20 reward is 157.27531061481335\n",
      "Round 123: Current mean reward -46.85297195261736 --- Top 20 reward is 143.36630356866968\n",
      "Round 124: Current mean reward -44.25245655428963 --- Top 20 reward is 143.14189317394286\n",
      "Round 125: Current mean reward -39.861532636007915 --- Top 20 reward is 158.3383911285776\n",
      "Round 126: Current mean reward -47.760992260447 --- Top 20 reward is 139.57831468365097\n",
      "Round 127: Current mean reward -47.64779849531208 --- Top 20 reward is 144.74062591522124\n",
      "Round 128: Current mean reward -47.70098870146445 --- Top 20 reward is 154.81723470763416\n",
      "Round 129: Current mean reward -48.822703892710976 --- Top 20 reward is 147.79867538140152\n",
      "Round 130: Current mean reward -44.746898893984664 --- Top 20 reward is 156.6480752758718\n",
      "Round 131: Current mean reward -52.30543827430861 --- Top 20 reward is 164.45970740721586\n",
      "Round 132: Current mean reward -47.50196596789116 --- Top 20 reward is 164.42038549461805\n",
      "Round 133: Current mean reward -45.704372426589785 --- Top 20 reward is 155.05842721166513\n",
      "Round 134: Current mean reward -48.78517692319852 --- Top 20 reward is 142.23170944868315\n",
      "Round 135: Current mean reward -48.3215337494011 --- Top 20 reward is 147.29593559357764\n",
      "Round 136: Current mean reward -50.557496470862056 --- Top 20 reward is 150.88469300369354\n",
      "Round 137: Current mean reward -45.59831694124536 --- Top 20 reward is 148.3408111790713\n",
      "Round 138: Current mean reward -41.6490317296106 --- Top 20 reward is 161.58160654400405\n",
      "Round 139: Current mean reward -41.679287232606825 --- Top 20 reward is 150.03458070537647\n",
      "Round 140: Current mean reward -48.600463822798666 --- Top 20 reward is 144.6209259607498\n",
      "Round 141: Current mean reward -48.29815251257579 --- Top 20 reward is 145.7492907610246\n",
      "Round 142: Current mean reward -49.571546316541294 --- Top 20 reward is 148.3551655254084\n",
      "Round 143: Current mean reward -50.1744880885926 --- Top 20 reward is 160.31314348688443\n",
      "Round 144: Current mean reward -46.126602632462756 --- Top 20 reward is 145.8861540233642\n",
      "Round 145: Current mean reward -55.90818863813273 --- Top 20 reward is 133.86332386261168\n",
      "Round 146: Current mean reward -53.6385330789959 --- Top 20 reward is 148.59849981592572\n",
      "Round 147: Current mean reward -47.86638672177283 --- Top 20 reward is 160.80445436172943\n",
      "Round 148: Current mean reward -47.750147166749294 --- Top 20 reward is 158.55000645032933\n",
      "Round 149: Current mean reward -58.923439153424965 --- Top 20 reward is 139.46558098909313\n",
      "Round 150: Current mean reward -56.11025890429614 --- Top 20 reward is 129.4176984602149\n",
      "Round 151: Current mean reward -53.482566118848595 --- Top 20 reward is 157.84950142151797\n",
      "Round 152: Current mean reward -54.48171800743663 --- Top 20 reward is 132.25078847945127\n",
      "Round 153: Current mean reward -50.4627527135404 --- Top 20 reward is 164.63472762604798\n",
      "Round 154: Current mean reward -51.12655629106044 --- Top 20 reward is 148.28431939382\n",
      "Round 155: Current mean reward -53.62120815478659 --- Top 20 reward is 156.9901058668284\n",
      "Round 156: Current mean reward -46.32150141074101 --- Top 20 reward is 173.32838839131077\n",
      "Round 157: Current mean reward -47.54768284377 --- Top 20 reward is 160.67082338047004\n",
      "Round 158: Current mean reward -48.2969772436261 --- Top 20 reward is 167.04362870692927\n",
      "Round 159: Current mean reward -44.71286706948245 --- Top 20 reward is 160.13460462960253\n",
      "Round 160: Current mean reward -50.701352160313526 --- Top 20 reward is 160.72982181463496\n",
      "Round 161: Current mean reward -48.82644235410856 --- Top 20 reward is 152.74356619779695\n",
      "Round 162: Current mean reward -47.02647495384689 --- Top 20 reward is 183.75930685856414\n",
      "Round 163: Current mean reward -48.59042584660357 --- Top 20 reward is 164.398089674613\n",
      "Round 164: Current mean reward -48.49027466131086 --- Top 20 reward is 153.84388298232102\n",
      "Round 165: Current mean reward -48.8480093516609 --- Top 20 reward is 160.50843120526787\n",
      "Round 166: Current mean reward -42.796173974102096 --- Top 20 reward is 160.66603337633813\n",
      "Round 167: Current mean reward -48.5457788919982 --- Top 20 reward is 149.4532207227757\n",
      "Round 168: Current mean reward -45.56694091338686 --- Top 20 reward is 158.6517832141234\n",
      "Round 169: Current mean reward -48.539178127829565 --- Top 20 reward is 154.30005220629067\n",
      "Round 170: Current mean reward -44.752428900372415 --- Top 20 reward is 158.15696889440034\n",
      "Round 171: Current mean reward -41.15786578527182 --- Top 20 reward is 163.64913429121506\n",
      "Round 172: Current mean reward -43.36426852859691 --- Top 20 reward is 151.23147526339935\n",
      "Round 173: Current mean reward -45.562409154826874 --- Top 20 reward is 140.1426421362829\n",
      "Round 174: Current mean reward -47.90230957708533 --- Top 20 reward is 153.2738877142176\n",
      "Round 175: Current mean reward -43.860795233569824 --- Top 20 reward is 156.94519608074262\n",
      "Round 176: Current mean reward -48.00705531095517 --- Top 20 reward is 163.1496339314429\n",
      "Round 177: Current mean reward -49.356911195720265 --- Top 20 reward is 142.41692786944185\n",
      "Round 178: Current mean reward -53.80175415697242 --- Top 20 reward is 126.04604043190261\n",
      "Round 179: Current mean reward -49.83112428894379 --- Top 20 reward is 160.93303521330418\n",
      "Round 180: Current mean reward -45.947416103791184 --- Top 20 reward is 145.29199402887724\n",
      "Round 181: Current mean reward -40.420628461290846 --- Top 20 reward is 149.16882772706037\n",
      "Round 182: Current mean reward -53.85079723782573 --- Top 20 reward is 155.7538160065777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 183: Current mean reward -52.46897580858551 --- Top 20 reward is 148.30977830401548\n",
      "Round 184: Current mean reward -51.63500649138921 --- Top 20 reward is 148.59714080294034\n",
      "Round 185: Current mean reward -52.789935330220416 --- Top 20 reward is 147.15127298361148\n",
      "Round 186: Current mean reward -56.8417518896267 --- Top 20 reward is 136.4630500867781\n",
      "Round 187: Current mean reward -51.87729325009027 --- Top 20 reward is 137.99982599493626\n",
      "Round 188: Current mean reward -52.7823977287062 --- Top 20 reward is 149.7384784175276\n",
      "Round 189: Current mean reward -54.958279604468906 --- Top 20 reward is 114.38947383976702\n",
      "Round 190: Current mean reward -55.91551631025822 --- Top 20 reward is 129.6880908517439\n",
      "Round 191: Current mean reward -44.84338312852009 --- Top 20 reward is 173.13008968522823\n",
      "Round 192: Current mean reward -47.836812801418496 --- Top 20 reward is 150.18610210178437\n",
      "Round 193: Current mean reward -45.220643580342504 --- Top 20 reward is 163.19761137707331\n",
      "Round 194: Current mean reward -50.467104680646415 --- Top 20 reward is 149.97731485518824\n",
      "Round 195: Current mean reward -45.958920607879676 --- Top 20 reward is 167.24309860831937\n",
      "Round 196: Current mean reward -48.890266573505606 --- Top 20 reward is 167.77149742709983\n",
      "Round 197: Current mean reward -54.06073437801086 --- Top 20 reward is 141.3905069590697\n",
      "Round 198: Current mean reward -56.1915157186753 --- Top 20 reward is 122.59926281923894\n",
      "Round 199: Current mean reward -58.26644786922581 --- Top 20 reward is 141.93800018629864\n",
      "Round 200: Current mean reward -59.9512804030681 --- Top 20 reward is 123.5762957915006\n",
      "Round 201: Current mean reward -56.047435248257585 --- Top 20 reward is 153.2359145612521\n",
      "Round 202: Current mean reward -56.94064124134124 --- Top 20 reward is 143.90641121998078\n",
      "Round 203: Current mean reward -56.84884677751545 --- Top 20 reward is 149.10882511658224\n",
      "Round 204: Current mean reward -57.800565419965565 --- Top 20 reward is 86.07271258244229\n",
      "Round 205: Current mean reward -55.90176690985116 --- Top 20 reward is 174.04515132737282\n",
      "Round 206: Current mean reward -58.141509498923966 --- Top 20 reward is 154.93630190523797\n",
      "Round 207: Current mean reward -64.1515814961499 --- Top 20 reward is 83.49363056861179\n",
      "Round 208: Current mean reward -56.459005483235906 --- Top 20 reward is 169.91959152264852\n",
      "Round 209: Current mean reward -59.636234079482314 --- Top 20 reward is 159.8049289756346\n",
      "Round 210: Current mean reward -58.59884424161176 --- Top 20 reward is 133.54926279031358\n",
      "Round 211: Current mean reward -57.85038393755279 --- Top 20 reward is 158.97336502503694\n",
      "Round 212: Current mean reward -42.84852693351758 --- Top 20 reward is 196.95850533699985\n",
      "Round 213: Current mean reward -58.06472491681583 --- Top 20 reward is 129.83115857977307\n",
      "Round 214: Current mean reward -54.5679463156439 --- Top 20 reward is 154.9752251609368\n",
      "Round 215: Current mean reward -62.20457289195655 --- Top 20 reward is 131.39465261761836\n",
      "Round 216: Current mean reward -54.250400650939596 --- Top 20 reward is 171.62876982758436\n",
      "Round 217: Current mean reward -60.05063874800602 --- Top 20 reward is 138.39858818320516\n",
      "Round 218: Current mean reward -56.85719971312848 --- Top 20 reward is 165.7505235027781\n",
      "Round 219: Current mean reward -58.14996500989711 --- Top 20 reward is 156.7483268375971\n",
      "Round 220: Current mean reward -49.14349836808302 --- Top 20 reward is 181.62749522566395\n",
      "Round 221: Current mean reward -58.83679278368606 --- Top 20 reward is 147.4919010255442\n",
      "Round 222: Current mean reward -51.91033947074979 --- Top 20 reward is 145.5344632619824\n",
      "Round 223: Current mean reward -55.6050756899293 --- Top 20 reward is 148.7399074718082\n"
     ]
    }
   ],
   "source": [
    "def nn_train(gpu = False, size_hidden_layer = 100, learningrate=1e-4, momentum = 0.5, mean_reward_threshold = 100, EPISODEN = 100,  STEPS_PER_EPISODE = 500):\n",
    "\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    env.seed(random_seed)\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, D_in, H, D_out):\n",
    "            super(Net, self).__init__()\n",
    "            self.linear1 = nn.Linear(D_in, H)\n",
    "            self.relu1 = torch.nn.ReLU()\n",
    "            self.linear2 = nn.Linear(H, D_out)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            In the forward function we accept a Tensor of input data and we must return\n",
    "            a Tensor of output data. We can use Modules defined in the constructor as\n",
    "            well as arbitrary operators on Tensors.\n",
    "            \"\"\"\n",
    "            l1 = self.linear1(x) \n",
    "            h_relu = self.relu1(l1)\n",
    "            #y_pred = nn.Softmax(self.linear2(h_relu))\n",
    "            y_pred = self.linear2(h_relu)\n",
    "            # y_pred = self.soft_max(self.linear2(h_relu))\n",
    "            return y_pred\n",
    "\n",
    "    def train(episodes_data):\n",
    "        model.train()\n",
    "\n",
    "        for episode in episodes_data:\n",
    "            for data, target in episode:\n",
    "                data = torch.tensor(data, device = device)\n",
    "                target = torch.tensor(target, device = device)\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(data)\n",
    "                y_pred = torch.unsqueeze(y_pred, 0)\n",
    "                target = torch.unsqueeze(target, 0)\n",
    "                #y_pred = torch.Tensor(y_pred, device=\"cpu\")\n",
    "                #y_pred = int(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "                # print(y_pred.shape)\n",
    "                loss = criterion(y_pred, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # print(f\"Loss for most recent episode is {loss.item()}\")\n",
    "\n",
    "    #diese methode kann zum erstellen von trainingdaten als auch für das \n",
    "    #testen des aktuellen modells verwendet werden\n",
    "    def generate_train_data(episoden, rendering):\n",
    "        model.eval()\n",
    "        all_scores = []\n",
    "        episodes = [] # [ [(state, action), (),...()], [(state, action), (),...()], ...]\n",
    "        rewards = []\n",
    "        sm = nn.Softmax(dim=0)\n",
    "        for i in range(episoden):\n",
    "            obs = env.reset() # reset for each new trial  \n",
    "            state_action_pairs_per_episode = [] #[(state, action),(), ...]\n",
    "            episode_reward = 0\n",
    "\n",
    "            for t in range(STEPS_PER_EPISODE): # run for maximum 500 timesteps or until done, whichever is first\n",
    "                #get states and put into network\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    data = torch.tensor(obs, device = device)\n",
    "                    outputs = model(data)\n",
    "#                     print(outputs)\n",
    "                    propability_dist = sm(outputs).cpu().detach().numpy()\n",
    "#                     print(propability_dist)\n",
    "                    outputs = outputs.cpu().detach().numpy()\n",
    "                    choice = np.random.choice(outputs, size=1, p=propability_dist)\n",
    "\n",
    "                    new_action = np.where((outputs == choice))[0][0] # new approach\n",
    "#                 new_action = int(np.argmax(outputs.cpu())) #old approach\n",
    "                state_action_tupel = (obs, new_action)\n",
    "                state_action_pairs_per_episode.append(state_action_tupel)\n",
    "        \n",
    "                if rendering:\n",
    "                    env.render()\n",
    "                obs, reward, done, info = env.step(new_action)\n",
    "                episode_reward+=reward\n",
    "                if not rendering:\n",
    "                    if done:\n",
    "                      #print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                        break\n",
    "\n",
    "                #print(episode_reward/STEPS_PER_EPISODE)\n",
    "                episodes.append(state_action_pairs_per_episode)\n",
    "                rewards.append(episode_reward)\n",
    "            env.close()\n",
    "\n",
    "\n",
    "        rewards = np.array(rewards)\n",
    "        episodes = np.array(episodes)\n",
    "        sort_index = rewards.argsort()\n",
    "        top_rewards = rewards[sort_index[::-1]][:20]\n",
    "        top_episodes = episodes[sort_index[::-1]][:20]\n",
    "        return top_episodes, top_rewards.mean(), rewards.mean()\n",
    "\n",
    "    D_in = 8\n",
    "    H = size_hidden_layer\n",
    "    D_out = 4\n",
    "    mean_reward = 0\n",
    "    all_reward = 0\n",
    "    round = 0 \n",
    "    \n",
    "    model = Net(D_in, H, D_out)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learningrate, momentum=momentum)\n",
    "    if gpu: \n",
    "        model.cuda()\n",
    "\n",
    "    \"\"\" Get initial training episodes  \"\"\"\n",
    "    train_data, top_rewards, all_reward = generate_train_data(EPISODEN, rendering = False)\n",
    "    print(\"Round -1 :Initial mean reward {} --- Initial top 20 reward is {}\".format(all_reward, top_rewards, round))\n",
    "    print(\"Start training...\")\n",
    "\n",
    "    \"\"\" Traing until mean reward hits threshold  \"\"\"\n",
    "    while (all_reward < mean_reward_threshold):\n",
    "        train(train_data)\n",
    "        train_data, top_rewards, all_reward = generate_train_data(EPISODEN, rendering = False)\n",
    "\n",
    "        print(\"Round {}: Current mean reward {} --- Top 20 reward is {}\".format(round, all_reward, top_rewards))\n",
    "        round = round + 1\n",
    "    \n",
    "    \"\"\" test with one final rendered epsiode  \"\"\"\n",
    "    waiting_mechanism = input(\"Press any key to continue. Please record the final episode\")\n",
    "    print(\"Start testing\")\n",
    "\n",
    "    train_data, top_rewards, all_reward = generate_train_data(1, rendering = True)\n",
    "    print(\"Current mean reward {}\".format(all_reward, top_rewards, round))\n",
    "        \n",
    "nn_train(gpu = True, \n",
    "         size_hidden_layer = 500, \n",
    "         learningrate=0.1, \n",
    "         momentum = 0.3, \n",
    "         mean_reward_threshold = 100, \n",
    "         EPISODEN = 100,  \n",
    "         STEPS_PER_EPISODE = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-9470bc58118a>:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(network(torch.from_numpy(np.array(state))))\n",
      "<ipython-input-1-9470bc58118a>:98: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  episodes = np.array(episodes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-227.72416459975923\n",
      "-209.233020043464\n",
      "-204.56590027019527\n",
      "-170.79569554088926\n",
      "-152.82717445588935\n",
      "-139.21471022686734\n",
      "-118.38977043713042\n",
      "-130.5544844234603\n",
      "-134.4211325287241\n",
      "-152.40297967024617\n",
      "-115.02294271443172\n",
      "-102.54474385845488\n",
      "-125.18068119522182\n",
      "-129.22903614830776\n",
      "-103.88511584823539\n",
      "-108.69902174343261\n",
      "-132.98083401895497\n",
      "-120.96077481169397\n",
      "-120.57095849999929\n",
      "-117.8733275001573\n",
      "-98.46196502457244\n",
      "-126.47690972070365\n",
      "-89.78581705656221\n",
      "-82.93342066098306\n",
      "-83.78237987889425\n",
      "-85.21313299925829\n",
      "-76.81274942461646\n",
      "-98.76807390744113\n",
      "-130.7047993853035\n",
      "-98.48865213457289\n",
      "-92.80659844894105\n",
      "-98.85247024502522\n",
      "-99.81275231452179\n",
      "-111.12236223272313\n",
      "-134.9168058589075\n",
      "-107.21057835404916\n",
      "-98.85592396983971\n",
      "-121.26695026725582\n",
      "-115.78625725817638\n",
      "-118.2333395447468\n",
      "-115.54881781769845\n",
      "-114.10614773277187\n",
      "-98.14885504535388\n",
      "-124.38127546428949\n",
      "-137.37627626838676\n",
      "-134.31500331282547\n",
      "-143.55041838954614\n",
      "-160.6299493188343\n",
      "-136.95609998127412\n",
      "-141.33794358801933\n",
      "-124.43144974202355\n",
      "-128.03306879011438\n",
      "-144.19843243578424\n",
      "-118.27726278040163\n",
      "-136.2698733124371\n",
      "-137.02401757084905\n",
      "-126.51114157476924\n",
      "-115.73660340880352\n",
      "-104.59097941553753\n",
      "-109.41458289819064\n",
      "-97.51222335792266\n",
      "-119.04991044305639\n",
      "-98.87001618307563\n",
      "-102.60305153267576\n",
      "-120.1912749103829\n",
      "-109.86760041130617\n",
      "-115.23043587003907\n",
      "-124.30949581177487\n",
      "-104.43353535225036\n",
      "-137.3937903203212\n",
      "-129.0889206137992\n",
      "-112.56664707571566\n",
      "-104.11888647321022\n",
      "-105.12407977378106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9470bc58118a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-9470bc58118a>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;31m#while (not done):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[1;31m# probabilities for action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[0mepisode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\piass\\anaconda3\\envs\\ki\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-9470bc58118a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mh_relu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_relu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#return F.softmax(y_pred, dim=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\piass\\anaconda3\\envs\\ki\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\piass\\anaconda3\\envs\\ki\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\piass\\anaconda3\\envs\\ki\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import gym\n",
    "# import random\n",
    "# import torch\n",
    "# import torchvision\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, D_in, H, D_out):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.linear1 = torch.nn.Linear(D_in, H)\n",
    "#         self.linear2 = torch.nn.Linear(H, D_out)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         h_relu = self.linear1(x).clamp(min=0)\n",
    "#         y_pred = self.linear2(h_relu)\n",
    "#         #return F.softmax(y_pred, dim=0)\n",
    "#         return y_pred\n",
    "    \n",
    "# def train(network, optimizer, criterion, top20):\n",
    "#     network.train()\n",
    "#     counter = 0\n",
    "    \n",
    "#     for episode in top20:\n",
    "#         for state, target in episode:\n",
    "#             optimizer.zero_grad()\n",
    "#             target = torch.tensor([target])\n",
    "#             output = network(torch.from_numpy(np.array(state))).unsqueeze(0)\n",
    "#             #if counter < 5:\n",
    "#             #    print(F.softmax(output), target)\n",
    "#             #    counter += 1\n",
    "#             #print(output.shape)\n",
    "#             #print(torch.tensor([target]))\n",
    "#             loss = criterion(output, target)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "            \n",
    "# def choose_action(output, epsilon):\n",
    "#     if random.random() > epsilon:\n",
    "#         return np.argmax(output.detach().numpy(), axis=0)\n",
    "#     else:\n",
    "#         return random.randint(0, 3)\n",
    "\n",
    "        \n",
    "\n",
    "# def main():\n",
    "#     env = gym.make(\"LunarLander-v2\")\n",
    "#     is_monitor_active = False\n",
    "    \n",
    "\n",
    "#     no_of_actions = env.action_space.n\n",
    "#     total_reward = 0\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "    \n",
    "#     num_episodes = 100\n",
    "#     num_steps = 500\n",
    "#     episodes = []\n",
    "#     plot_data = []\n",
    "    \n",
    "#     network = Net(8, 256, 4)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.SGD(network.parameters(), momentum=0.9, lr=0.00015)\n",
    "    \n",
    "#     while(len(episodes) == 0 or episodes[:,1].mean() <= 100):\n",
    "#     #for i in range(100):\n",
    "#         episodes = []\n",
    "#         for e in range(num_episodes):\n",
    "#             episode = []\n",
    "#             reward_e = 0\n",
    "#             for s in range(num_steps):\n",
    "                \n",
    "#             #while (not done):\n",
    "#                 # probabilities for action\n",
    "#                 output = F.softmax(network(torch.from_numpy(np.array(state))))\n",
    "#                 action =  random.choices([0,1,2,3], output, k=1)[0]\n",
    "#                 episode.append((state, action))\n",
    "#                 state, reward, done, _ = env.step(action)\n",
    "#                 reward_e += reward\n",
    "#                 last_reward = reward\n",
    "\n",
    "#                 if done:\n",
    "#                     #reward_e /= s\n",
    "#                     break\n",
    "\n",
    "#             episodes.append((episode, reward_e, last_reward))\n",
    "#             if is_monitor_active:\n",
    "#                 time.sleep(0.1)\n",
    "#             state = env.reset()\n",
    "#             done = False\n",
    "            \n",
    "\n",
    "#         episodes = np.array(episodes)\n",
    "#         mean = episodes[:,1].mean()\n",
    "#         #if (mean > 50) and (not is_monitor_active):\n",
    "#         #    env = gym.wrappers.Monitor(env, \"recording_lunar\", force=True)\n",
    "#         #    time.sleep(1)\n",
    "#         #    state = env.reset()\n",
    "#         #    is_monitor_active = True\n",
    "            \n",
    "#         plot_data.append(mean)\n",
    "#         print(mean)\n",
    "\n",
    "#         episodes_sorted = episodes[episodes[:,1].argsort()]\n",
    "#         top20 = episodes_sorted[80:]\n",
    "#         #print(top20[:,1])\n",
    "#         train(network, optimizer, criterion, top20[:,0])\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.xlabel(\"episode\")\n",
    "#     plt.ylabel(\"mean reward\")\n",
    "#     plt.plot(plot_data, label=\"100 Hidden layer, lr=0.1\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ReinforcementLearning_lunarlanding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
