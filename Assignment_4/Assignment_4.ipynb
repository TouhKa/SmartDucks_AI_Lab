{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgvvWbigDxeN",
    "outputId": "5e662a23-27ce-4ad8-e1c2-92d220aec7e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards, round):\n",
    "    plt.plot(range(0,round + 1), rewards)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Mean Reward\")\n",
    "    plt.title(\"Mean Reward per Episode\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "Tz99vn4VMqOd",
    "outputId": "8fbd44ef-54ae-43e4-efb9-d85d8d332126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round -1 :Initial mean reward -194.01329909115947 --- Initial top 20 reward is -74.0963727311935\n",
      "Start training...\n",
      "Round 0: Current mean reward -126.87497605424842 --- Top 20 reward is -71.88651725057994\n",
      "Round 1: Current mean reward -98.97527289392065 --- Top 20 reward is -73.95753308923808\n",
      "Round 2: Current mean reward -97.614251686907 --- Top 20 reward is -67.15633479618234\n",
      "Round 3: Current mean reward -107.66414338474688 --- Top 20 reward is -76.91317947978358\n",
      "Round 4: Current mean reward -104.57169962751142 --- Top 20 reward is -77.70056009064288\n",
      "Round 5: Current mean reward -98.58600755172583 --- Top 20 reward is -70.34231959332509\n",
      "Round 6: Current mean reward -98.60927783900225 --- Top 20 reward is -73.79955782625359\n",
      "Round 7: Current mean reward -92.61992838447169 --- Top 20 reward is -63.660242157514894\n",
      "Round 8: Current mean reward -95.73131983908426 --- Top 20 reward is -70.9318856323034\n",
      "Round 9: Current mean reward -97.31423645863963 --- Top 20 reward is -77.68201009245176\n",
      "Round 10: Current mean reward -102.80106070358961 --- Top 20 reward is -85.63333747246584\n",
      "Round 11: Current mean reward -100.06387182665985 --- Top 20 reward is -76.90589448126777\n",
      "Round 12: Current mean reward -103.00128067040923 --- Top 20 reward is -77.89017091546603\n",
      "Round 13: Current mean reward -96.60161553327633 --- Top 20 reward is -70.61036729493456\n",
      "Round 14: Current mean reward -102.59534427799656 --- Top 20 reward is -68.7112945484128\n",
      "Round 15: Current mean reward -90.2431087300275 --- Top 20 reward is -60.945725377384406\n",
      "Round 16: Current mean reward -88.86223184049685 --- Top 20 reward is -54.298172353047164\n",
      "Round 17: Current mean reward -81.40073881759403 --- Top 20 reward is -43.428768925202135\n",
      "Round 18: Current mean reward -80.7160330445222 --- Top 20 reward is -41.67155515516307\n",
      "Round 19: Current mean reward -79.30769812223998 --- Top 20 reward is -39.34829635387506\n",
      "Round 20: Current mean reward -80.98036995646812 --- Top 20 reward is -42.11428067538574\n",
      "Round 21: Current mean reward -82.78399684094917 --- Top 20 reward is -49.090072840492134\n",
      "Round 22: Current mean reward -77.87717472514522 --- Top 20 reward is -45.46424238076616\n",
      "Round 23: Current mean reward -83.88135867156642 --- Top 20 reward is -52.5461508224882\n",
      "Round 24: Current mean reward -82.36904780839183 --- Top 20 reward is -54.727961250222926\n",
      "Round 25: Current mean reward -80.79264923486018 --- Top 20 reward is -49.210556633698765\n",
      "Round 26: Current mean reward -76.31940844209403 --- Top 20 reward is -47.37249359164645\n",
      "Round 27: Current mean reward -80.1612937767063 --- Top 20 reward is -51.855836873175136\n",
      "Round 28: Current mean reward -75.78668358462593 --- Top 20 reward is -47.54290460648954\n",
      "Round 29: Current mean reward -75.64421752533138 --- Top 20 reward is -50.22286776046086\n",
      "Round 30: Current mean reward -77.02246901158894 --- Top 20 reward is -49.428059767701775\n",
      "Round 31: Current mean reward -74.8174559925579 --- Top 20 reward is -48.54116014900045\n",
      "Round 32: Current mean reward -86.01516570680973 --- Top 20 reward is -52.78766621870507\n",
      "Round 33: Current mean reward -75.34885248345229 --- Top 20 reward is -44.53755627161574\n",
      "Round 34: Current mean reward -80.16893421524065 --- Top 20 reward is -53.390574082943736\n",
      "Round 35: Current mean reward -82.82636438936075 --- Top 20 reward is -55.10578985976569\n",
      "Round 36: Current mean reward -74.83751867526122 --- Top 20 reward is -46.5552331944854\n",
      "Round 37: Current mean reward -73.33223867429427 --- Top 20 reward is -43.17376353880493\n",
      "Round 38: Current mean reward -79.07789324233967 --- Top 20 reward is -49.51679747178274\n",
      "Round 39: Current mean reward -77.61577725594405 --- Top 20 reward is -46.747052301448385\n",
      "Round 40: Current mean reward -75.87129261429166 --- Top 20 reward is -41.138606483294396\n",
      "Round 41: Current mean reward -75.9275393875279 --- Top 20 reward is -43.326955095560194\n",
      "Round 42: Current mean reward -62.860301708968834 --- Top 20 reward is -35.451766909109665\n",
      "Round 43: Current mean reward -58.23176024159431 --- Top 20 reward is -28.281625252589272\n",
      "Round 44: Current mean reward -60.059924627101154 --- Top 20 reward is -29.757526394973222\n",
      "Round 45: Current mean reward -61.90979415061611 --- Top 20 reward is -30.514407971375636\n",
      "Round 46: Current mean reward -63.03659881462193 --- Top 20 reward is -37.32559942526311\n",
      "Round 47: Current mean reward -55.99873214411377 --- Top 20 reward is -28.146112690489637\n",
      "Round 48: Current mean reward -47.618073270676234 --- Top 20 reward is -20.271753971837732\n",
      "Round 49: Current mean reward -51.03346461510128 --- Top 20 reward is -31.108970030651257\n",
      "Round 50: Current mean reward -52.2855351262273 --- Top 20 reward is -27.062116343836312\n",
      "Round 51: Current mean reward -44.45951844038931 --- Top 20 reward is -20.408968394389\n",
      "Round 52: Current mean reward -42.57663633323143 --- Top 20 reward is -17.846176106790846\n",
      "Round 53: Current mean reward -39.98328273730532 --- Top 20 reward is -18.533468421694288\n",
      "Round 54: Current mean reward -36.864206146504664 --- Top 20 reward is -12.050254445109413\n",
      "Round 55: Current mean reward -33.3070016096208 --- Top 20 reward is -10.888685587941485\n",
      "Round 56: Current mean reward -28.018872248225918 --- Top 20 reward is -4.9858886298287866\n",
      "Round 57: Current mean reward -24.81510140504304 --- Top 20 reward is -2.5003439994140506\n",
      "Round 58: Current mean reward -18.17696135835903 --- Top 20 reward is 7.427519280137995\n",
      "Round 59: Current mean reward -10.067440492042476 --- Top 20 reward is 15.052350145731372\n",
      "Round 60: Current mean reward -13.211487369580333 --- Top 20 reward is 13.234591281689092\n",
      "Round 61: Current mean reward -2.064168890391889 --- Top 20 reward is 30.80208754476799\n",
      "Round 62: Current mean reward 9.304976742251785 --- Top 20 reward is 83.8433507507104\n",
      "Round 63: Current mean reward 9.887070067697762 --- Top 20 reward is 67.38359054520089\n",
      "Round 64: Current mean reward -3.8159121800227616 --- Top 20 reward is 29.226199973453987\n",
      "Round 65: Current mean reward -2.258828446038689 --- Top 20 reward is 28.339435297388423\n",
      "Round 66: Current mean reward 1.50224904743254 --- Top 20 reward is 40.72053039243687\n",
      "Round 67: Current mean reward 10.976975855120578 --- Top 20 reward is 81.62912980631121\n",
      "Round 68: Current mean reward 27.41523905328377 --- Top 20 reward is 121.10058259775212\n",
      "Round 69: Current mean reward 36.3575470878878 --- Top 20 reward is 151.18776728425433\n",
      "Round 70: Current mean reward 19.334501054499277 --- Top 20 reward is 78.62241300603135\n",
      "Round 71: Current mean reward 20.943312817427717 --- Top 20 reward is 88.08346265319962\n",
      "Round 72: Current mean reward 11.817012775912914 --- Top 20 reward is 62.07589427038645\n",
      "Round 73: Current mean reward 11.942148697568081 --- Top 20 reward is 66.33134053555882\n",
      "Round 74: Current mean reward 26.385347448975264 --- Top 20 reward is 117.04159416404711\n",
      "Round 75: Current mean reward 30.497426046425115 --- Top 20 reward is 139.32830849112509\n",
      "Round 76: Current mean reward 15.632360621256938 --- Top 20 reward is 60.01479069316588\n",
      "Round 77: Current mean reward 8.4468830579413 --- Top 20 reward is 59.61336521297526\n",
      "Round 78: Current mean reward 9.381877809459656 --- Top 20 reward is 54.659198331999036\n",
      "Round 79: Current mean reward 9.291199860805325 --- Top 20 reward is 63.01994872881802\n",
      "Round 80: Current mean reward 17.50665460478884 --- Top 20 reward is 84.54332855217962\n",
      "Round 81: Current mean reward 24.773254154143242 --- Top 20 reward is 119.84325601948676\n",
      "Round 82: Current mean reward 25.89271965919098 --- Top 20 reward is 124.63459602052114\n",
      "Round 83: Current mean reward 44.596789768464404 --- Top 20 reward is 172.72853142558705\n",
      "Round 84: Current mean reward 28.144190984730656 --- Top 20 reward is 106.65351611632805\n",
      "Round 85: Current mean reward 28.024038066280255 --- Top 20 reward is 126.10044228269271\n",
      "Round 86: Current mean reward 10.543760264535495 --- Top 20 reward is 101.83279425564838\n",
      "Round 87: Current mean reward 7.346563618512687 --- Top 20 reward is 79.30095073961633\n",
      "Round 88: Current mean reward -1.5673348393206334 --- Top 20 reward is 60.94212992228828\n",
      "Round 89: Current mean reward -1.9986630620728112 --- Top 20 reward is 63.23335102023577\n",
      "Round 90: Current mean reward 20.638173088825532 --- Top 20 reward is 122.4845665442876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 91: Current mean reward 11.083479303929373 --- Top 20 reward is 100.1606477728361\n",
      "Round 92: Current mean reward 12.294794651656343 --- Top 20 reward is 115.4979967149779\n",
      "Round 93: Current mean reward 26.945338687454754 --- Top 20 reward is 137.1157218471562\n",
      "Round 94: Current mean reward 46.17014984493163 --- Top 20 reward is 166.4880961634149\n",
      "Round 95: Current mean reward 64.70935824680755 --- Top 20 reward is 201.66361957791682\n",
      "Round 96: Current mean reward 87.08267807956965 --- Top 20 reward is 240.60640296510755\n",
      "Round 97: Current mean reward 69.96749426534629 --- Top 20 reward is 223.81733421877212\n",
      "Round 98: Current mean reward 90.99926758703332 --- Top 20 reward is 247.36754773832203\n",
      "Round 99: Current mean reward 75.2245378627573 --- Top 20 reward is 217.53268111353478\n",
      "Round 100: Current mean reward 56.59907278236318 --- Top 20 reward is 208.22850746677574\n",
      "Round 101: Current mean reward 85.86444595160478 --- Top 20 reward is 227.40673896523964\n",
      "Round 102: Current mean reward 85.87038141991682 --- Top 20 reward is 225.37551839706845\n",
      "Round 103: Current mean reward 98.00519711070636 --- Top 20 reward is 252.33713610280378\n",
      "Round 104: Current mean reward 89.37308611066932 --- Top 20 reward is 251.49891646277834\n",
      "Round 105: Current mean reward 76.84300960346775 --- Top 20 reward is 235.04898652249213\n",
      "Round 106: Current mean reward 66.56274280364477 --- Top 20 reward is 230.20597974105976\n",
      "Round 107: Current mean reward 63.0424514647829 --- Top 20 reward is 243.96861096994516\n",
      "Round 108: Current mean reward 57.75170036214571 --- Top 20 reward is 211.94660429514087\n",
      "Round 109: Current mean reward 69.36090061344305 --- Top 20 reward is 226.8436985676348\n",
      "Round 110: Current mean reward 72.91356074223752 --- Top 20 reward is 230.081710146679\n",
      "Round 111: Current mean reward 68.0674872856255 --- Top 20 reward is 221.74694682575583\n",
      "Round 112: Current mean reward 98.78162993667705 --- Top 20 reward is 258.96411556555415\n",
      "Round 113: Current mean reward 91.56633288994614 --- Top 20 reward is 255.0254164647643\n",
      "Round 114: Current mean reward 67.11284414809678 --- Top 20 reward is 239.81213880347363\n",
      "Round 115: Current mean reward 81.4166014929235 --- Top 20 reward is 258.05691908619343\n",
      "Round 116: Current mean reward 96.62131108300824 --- Top 20 reward is 252.32898133319455\n",
      "Round 117: Current mean reward 112.29278787731032 --- Top 20 reward is 267.6023170072367\n",
      "Press any key to continue. Please record the final episode\n",
      "Start testing\n",
      "Current mean reward 365.7620566503427\n"
     ]
    }
   ],
   "source": [
    "def nn_train(gpu = False, size_hidden_layer = 100, learningrate=1e-4, momentum = 0.5, mean_reward_threshold = 100, EPISODEN = 100,  STEPS_PER_EPISODE = 500):\n",
    "\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    env = gym.wrappers.Monitor(env, \"recording_lunar\", force=True)\n",
    "    env.seed(random_seed)\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, D_in, H, D_out):\n",
    "            super(Net, self).__init__()\n",
    "            self.linear1 = nn.Linear(D_in, H)\n",
    "            self.relu1 = torch.nn.ReLU()\n",
    "            self.linear2 = nn.Linear(H, D_out)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            In the forward function we accept a Tensor of input data and we must return\n",
    "            a Tensor of output data. We can use Modules defined in the constructor as\n",
    "            well as arbitrary operators on Tensors.\n",
    "            \"\"\"\n",
    "            l1 = self.linear1(x) \n",
    "            h_relu = self.relu1(l1)\n",
    "            y_pred = self.linear2(h_relu)\n",
    "            return y_pred\n",
    "\n",
    "    def train(episodes_data):\n",
    "        model.train()\n",
    "\n",
    "        for episode in episodes_data:\n",
    "            for data, target in episode:\n",
    "                data = torch.tensor(data, device = device)\n",
    "                target = torch.tensor(target, device = device)\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(data)\n",
    "                y_pred = torch.unsqueeze(y_pred, 0)\n",
    "                target = torch.unsqueeze(target, 0)\n",
    "\n",
    "                loss = criterion(y_pred, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "\n",
    "    #diese methode kann zum erstellen von trainingdaten als auch für das \n",
    "    #testen des aktuellen modells verwendet werden\n",
    "    def generate_train_data(episoden, rendering):\n",
    "        model.eval()\n",
    "        all_scores = []\n",
    "        episodes = [] # [ [(state, action), (),...()], [(state, action), (),...()], ...]\n",
    "        rewards = []\n",
    "        sm = nn.Softmax(dim=0)\n",
    "        for i in range(episoden):\n",
    "            obs = env.reset() # reset for each new trial  \n",
    "            state_action_pairs_per_episode = [] #[(state, action),(), ...]\n",
    "            episode_reward = 0\n",
    "\n",
    "            for t in range(STEPS_PER_EPISODE): # run for maximum 500 timesteps or until done, whichever is first               \n",
    "                with torch.no_grad():\n",
    "                    data = torch.tensor(obs, device = device)\n",
    "                    outputs = model(data)\n",
    "                    propability_dist = sm(outputs).cpu().detach().numpy()\n",
    "                    outputs = outputs.cpu().detach().numpy()\n",
    "                    choice = np.random.choice(outputs, size=1, p=propability_dist)\n",
    "                    new_action = np.where((outputs == choice))[0][0]\n",
    "\n",
    "                state_action_tupel = (obs, new_action)\n",
    "                state_action_pairs_per_episode.append(state_action_tupel)\n",
    "        \n",
    "                if rendering:\n",
    "                    env.render()\n",
    "                obs, reward, done, info = env.step(new_action)\n",
    "                episode_reward+=reward\n",
    "                if not rendering:\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "            episodes.append(state_action_pairs_per_episode)\n",
    "            rewards.append(episode_reward)\n",
    "            env.close()\n",
    "\n",
    "\n",
    "        rewards = np.array(rewards)\n",
    "        episodes = np.array(episodes)\n",
    "        sort_index = rewards.argsort()\n",
    "        top_rewards = rewards[sort_index[::-1]][:20]\n",
    "        top_episodes = episodes[sort_index[::-1]][:20]\n",
    "        return top_episodes, top_rewards.mean(), rewards.mean()\n",
    "\n",
    "    D_in = 8\n",
    "    H = size_hidden_layer\n",
    "    D_out = 4\n",
    "    mean_reward = 0\n",
    "    all_reward = 0\n",
    "    round = 0 \n",
    "    \n",
    "    model = Net(D_in, H, D_out)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learningrate, momentum=momentum)\n",
    "    if gpu: \n",
    "        model.cuda()\n",
    "\n",
    "    \"\"\" Get initial training episodes  \"\"\"\n",
    "    train_data, top_rewards, all_reward = generate_train_data(EPISODEN, rendering = False)\n",
    "    print(\"Round -1 :Initial mean reward {} --- Initial top 20 reward is {}\".format(all_reward, top_rewards, round))\n",
    "    print(\"Start training...\")\n",
    "\n",
    "    \"\"\" Traing until mean reward hits threshold  \"\"\"\n",
    "    while (all_reward < mean_reward_threshold):\n",
    "        train(train_data)\n",
    "        train_data, top_rewards, all_reward = generate_train_data(EPISODEN, rendering = False)\n",
    "\n",
    "        print(\"Round {}: Current mean reward {} --- Top 20 reward is {}\".format(round, all_reward, top_rewards))\n",
    "        round = round + 1\n",
    "    \n",
    "    \"\"\" test with one final rendered epsiode  \"\"\"\n",
    "    waiting_mechanism = input(\"Press any key to continue. Please record the final episode\")\n",
    "    print(\"Start testing\")\n",
    "\n",
    "    train_data, top_rewards, all_reward = generate_train_data(1, rendering = True)\n",
    "    print(\"Current mean reward {}\".format(all_reward, top_rewards, round))\n",
    "        \n",
    "nn_train(gpu = True, \n",
    "         size_hidden_layer = 500, \n",
    "         learningrate=0.001, \n",
    "         momentum = 0.9, \n",
    "         mean_reward_threshold = 100, \n",
    "         EPISODEN = 100,  \n",
    "         STEPS_PER_EPISODE = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ReinforcementLearning_lunarlanding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
